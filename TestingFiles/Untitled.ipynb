{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 18:05:30.829979: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-19 18:05:30.853214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 18:05:31.214811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler, TextToVideoSDPipeline, UNet3DConditionModel,StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6f54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TextToVideoSDPipeline in module diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_synth:\n",
      "\n",
      "class TextToVideoSDPipeline(diffusers.pipelines.pipeline_utils.DiffusionPipeline, diffusers.loaders.TextualInversionLoaderMixin, diffusers.loaders.LoraLoaderMixin)\n",
      " |  TextToVideoSDPipeline(vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_3d_condition.UNet3DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers)\n",
      " |  \n",
      " |  Pipeline for text-to-video generation.\n",
      " |  \n",
      " |  This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
      " |  implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
      " |  \n",
      " |  Args:\n",
      " |      vae ([`AutoencoderKL`]):\n",
      " |          Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
      " |      text_encoder ([`CLIPTextModel`]):\n",
      " |          Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n",
      " |      tokenizer (`CLIPTokenizer`):\n",
      " |          A [`~transformers.CLIPTokenizer`] to tokenize text.\n",
      " |      unet ([`UNet3DConditionModel`]):\n",
      " |          A [`UNet3DConditionModel`] to denoise the encoded video latents.\n",
      " |      scheduler ([`SchedulerMixin`]):\n",
      " |          A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
      " |          [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextToVideoSDPipeline\n",
      " |      diffusers.pipelines.pipeline_utils.DiffusionPipeline\n",
      " |      diffusers.configuration_utils.ConfigMixin\n",
      " |      diffusers.utils.hub_utils.PushToHubMixin\n",
      " |      diffusers.loaders.TextualInversionLoaderMixin\n",
      " |      diffusers.loaders.LoraLoaderMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: Union[str, List[str]] = None, height: Optional[int] = None, width: Optional[int] = None, num_frames: int = 16, num_inference_steps: int = 50, guidance_scale: float = 9.0, negative_prompt: Union[str, List[str], NoneType] = None, eta: float = 0.0, generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None, latents: Optional[torch.FloatTensor] = None, prompt_embeds: Optional[torch.FloatTensor] = None, negative_prompt_embeds: Optional[torch.FloatTensor] = None, output_type: Optional[str] = 'np', return_dict: bool = True, callback: Optional[Callable[[int, int, torch.FloatTensor], NoneType]] = None, callback_steps: int = 1, cross_attention_kwargs: Optional[Dict[str, Any]] = None)\n",
      " |          The call function to the pipeline for generation.\n",
      " |      \n",
      " |          Args:\n",
      " |              prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n",
      " |              height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
      " |                  The height in pixels of the generated video.\n",
      " |              width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
      " |                  The width in pixels of the generated video.\n",
      " |              num_frames (`int`, *optional*, defaults to 16):\n",
      " |                  The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds\n",
      " |                  amounts to 2 seconds of video.\n",
      " |              num_inference_steps (`int`, *optional*, defaults to 50):\n",
      " |                  The number of denoising steps. More denoising steps usually lead to a higher quality videos at the\n",
      " |                  expense of slower inference.\n",
      " |              guidance_scale (`float`, *optional*, defaults to 7.5):\n",
      " |                  A higher guidance scale value encourages the model to generate images closely linked to the text\n",
      " |                  `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n",
      " |              negative_prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n",
      " |                  pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n",
      " |              num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
      " |                  The number of images to generate per prompt.\n",
      " |              eta (`float`, *optional*, defaults to 0.0):\n",
      " |                  Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
      " |                  to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n",
      " |              generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
      " |                  A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
      " |                  generation deterministic.\n",
      " |              latents (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for video\n",
      " |                  generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
      " |                  tensor is generated by sampling using the supplied random `generator`. Latents should be of shape\n",
      " |                  `(batch_size, num_channel, num_frames, height, width)`.\n",
      " |              prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n",
      " |                  provided, text embeddings are generated from the `prompt` input argument.\n",
      " |              negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n",
      " |                  not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n",
      " |              output_type (`str`, *optional*, defaults to `\"np\"`):\n",
      " |                  The output format of the generated video. Choose between `torch.FloatTensor` or `np.array`.\n",
      " |              return_dict (`bool`, *optional*, defaults to `True`):\n",
      " |                  Whether or not to return a [`~pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`] instead\n",
      " |                  of a plain tuple.\n",
      " |              callback (`Callable`, *optional*):\n",
      " |                  A function that calls every `callback_steps` steps during inference. The function is called with the\n",
      " |                  following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
      " |              callback_steps (`int`, *optional*, defaults to 1):\n",
      " |                  The frequency at which the `callback` function is called. If not specified, the callback is called at\n",
      " |                  every step.\n",
      " |              cross_attention_kwargs (`dict`, *optional*):\n",
      " |                  A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n",
      " |                  [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |          ```py\n",
      " |          >>> import torch\n",
      " |          >>> from diffusers import TextToVideoSDPipeline\n",
      " |          >>> from diffusers.utils import export_to_video\n",
      " |      \n",
      " |          >>> pipe = TextToVideoSDPipeline.from_pretrained(\n",
      " |          ...     \"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\"\n",
      " |          ... )\n",
      " |          >>> pipe.enable_model_cpu_offload()\n",
      " |      \n",
      " |          >>> prompt = \"Spiderman is surfing\"\n",
      " |          >>> video_frames = pipe(prompt).frames\n",
      " |          >>> video_path = export_to_video(video_frames)\n",
      " |          >>> video_path\n",
      " |          ```\n",
      " |      \n",
      " |      \n",
      " |          Returns:\n",
      " |              [`~pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`] or `tuple`:\n",
      " |                  If `return_dict` is `True`, [`~pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput`] is\n",
      " |                  returned, otherwise a `tuple` is returned where the first element is a list with the generated frames.\n",
      " |  \n",
      " |  __init__(self, vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_3d_condition.UNet3DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_inputs(self, prompt, height, width, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.check_inputs\n",
      " |  \n",
      " |  decode_latents(self, latents)\n",
      " |  \n",
      " |  disable_vae_slicing(self)\n",
      " |      Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  disable_vae_tiling(self)\n",
      " |      Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  enable_model_cpu_offload(self, gpu_id=0)\n",
      " |      Offload all models to CPU to reduce memory usage with a low impact on performance. Moves one whole model at a\n",
      " |      time to the GPU when its `forward` method is called, and the model remains in GPU until the next model runs.\n",
      " |      Memory savings are lower than using `enable_sequential_cpu_offload`, but performance is much better due to the\n",
      " |      iterative execution of the `unet`.\n",
      " |  \n",
      " |  enable_vae_slicing(self)\n",
      " |      Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n",
      " |      compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n",
      " |  \n",
      " |  enable_vae_tiling(self)\n",
      " |      Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n",
      " |      compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n",
      " |      processing larger images.\n",
      " |  \n",
      " |  prepare_extra_step_kwargs(self, generator, eta)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n",
      " |  \n",
      " |  prepare_latents(self, batch_size, num_channels_latents, num_frames, height, width, dtype, device, generator, latents=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Any)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  disable_attention_slicing(self)\n",
      " |      Disable sliced attention computation. If `enable_attention_slicing` was previously called, attention is\n",
      " |      computed in one step.\n",
      " |  \n",
      " |  disable_xformers_memory_efficient_attention(self)\n",
      " |      Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n",
      " |  \n",
      " |  enable_attention_slicing(self, slice_size: Union[str, int, NoneType] = 'auto')\n",
      " |      Enable sliced attention computation. When this option is enabled, the attention module splits the input tensor\n",
      " |      in slices to compute attention in several steps. For more than one attention head, the computation is performed\n",
      " |      sequentially over each head. This is useful to save some memory in exchange for a small speed decrease.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      â ï¸ Don't enable attention slicing if you're already using `scaled_dot_product_attention` (SDPA) from PyTorch\n",
      " |      2.0 or xFormers. These attention computations are already very memory efficient so you won't need to enable\n",
      " |      this function. If you enable attention slicing with SDPA or xFormers, it can lead to serious slow downs!\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
      " |              When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
      " |              `\"max\"`, maximum amount of memory will be saved by running only one slice at a time. If a number is\n",
      " |              provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n",
      " |              must be a multiple of `slice_size`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import StableDiffusionPipeline\n",
      " |      \n",
      " |      >>> pipe = StableDiffusionPipeline.from_pretrained(\n",
      " |      ...     \"runwayml/stable-diffusion-v1-5\",\n",
      " |      ...     torch_dtype=torch.float16,\n",
      " |      ...     use_safetensors=True,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n",
      " |      >>> pipe.enable_attention_slicing()\n",
      " |      >>> image = pipe(prompt).images[0]\n",
      " |      ```\n",
      " |  \n",
      " |  enable_sequential_cpu_offload(self, gpu_id: int = 0, device: Union[torch.device, str] = 'cuda')\n",
      " |      Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n",
      " |      text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n",
      " |      `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n",
      " |      Note that offloading happens on a submodule basis. Memory savings are higher than with\n",
      " |      `enable_model_cpu_offload`, but performance is lower.\n",
      " |  \n",
      " |  enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None)\n",
      " |      Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/). When this\n",
      " |      option is enabled, you should observe lower GPU memory usage and a potential speed up during inference. Speed\n",
      " |      up during training is not guaranteed.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      â ï¸ When memory efficient attention and sliced attention are both enabled, memory efficient attention takes\n",
      " |      precedent.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          attention_op (`Callable`, *optional*):\n",
      " |              Override the default `None` operator for use as `op` argument to the\n",
      " |              [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n",
      " |              function of xFormers.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
      " |      \n",
      " |      >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
      " |      >>> pipe = pipe.to(\"cuda\")\n",
      " |      >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
      " |      >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n",
      " |      >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
      " |      ```\n",
      " |  \n",
      " |  progress_bar(self, iterable=None, total=None)\n",
      " |  \n",
      " |  register_modules(self, **kwargs)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, variant: Optional[str] = None, push_to_hub: bool = False, **kwargs)\n",
      " |      Save all saveable variables of the pipeline to a directory. A pipeline variable can be saved and loaded if its\n",
      " |      class implements both a save and loading method. The pipeline is easily reloaded using the\n",
      " |      [`~DiffusionPipeline.from_pretrained`] class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save a pipeline to. Will be created if it doesn't exist.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  set_attention_slice(self, slice_size: Optional[int])\n",
      " |  \n",
      " |  set_progress_bar_config(self, **kwargs)\n",
      " |  \n",
      " |  set_use_memory_efficient_attention_xformers(self, valid: bool, attention_op: Optional[Callable] = None) -> None\n",
      " |  \n",
      " |  to(self, torch_device: Union[str, torch.device, NoneType] = None, torch_dtype: Optional[torch.dtype] = None, silence_dtype_warnings: bool = False)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  download(pretrained_model_name, **kwargs) -> Union[str, os.PathLike] from builtins.type\n",
      " |      Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name (`str` or `os.PathLike`, *optional*):\n",
      " |              A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |              hosted on the Hub.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called `pipeline.py` that defines\n",
      " |                    the custom pipeline.\n",
      " |      \n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current `main` branch of GitHub.\n",
      " |      \n",
      " |                  - A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              ð§ª This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, take a look at [How to contribute a\n",
      " |              community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a ð¤ Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      " |              weights. If set to `False`, safetensors weights are not loaded.\n",
      " |          use_onnx (`bool`, *optional*, defaults to `False`):\n",
      " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
      " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
      " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
      " |              with `.onnx` and `.pb`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `os.PathLike`:\n",
      " |              A path to the downloaded pipeline.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) from builtins.type\n",
      " |      Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
      " |      \n",
      " |      If you get the error message below, you need to finetune the weights for your downstream task:\n",
      " |      \n",
      " |      ```\n",
      " |      Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      " |      - conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
      " |      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " |      ```\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |                    hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing pipeline weights\n",
      " |                    saved using\n",
      " |                  [`~DiffusionPipeline.save_pretrained`].\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model with another dtype. If \"auto\" is passed, the\n",
      " |              dtype is automatically derived from the model's weights.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              ð§ª This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`) of a custom\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called pipeline.py that defines\n",
      " |                    the custom pipeline.\n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current main branch of GitHub.\n",
      " |                  - A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
      " |              Adding Custom\n",
      " |              Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a ð¤ Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if youâre downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      " |              A map that specifies where each submodule should go. It doesnât need to be defined for each\n",
      " |              parameter/buffer name; once a given module name is inside, every submodule of it will be sent to the\n",
      " |              same device.\n",
      " |      \n",
      " |              Set `device_map=\"auto\"` to have ð¤ Accelerate automatically compute the most optimized `device_map`. For\n",
      " |              more information about each option see [designing a device\n",
      " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      " |          max_memory (`Dict`, *optional*):\n",
      " |              A dictionary device identifier for the maximum memory. Will default to the maximum memory available for\n",
      " |              each GPU and the available CPU RAM if unset.\n",
      " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
      " |              The path to offload weights if device_map contains the value `\"disk\"`.\n",
      " |          offload_state_dict (`bool`, *optional*):\n",
      " |              If `True`, temporarily offloads the CPU state dict to the hard drive to avoid running out of CPU RAM if\n",
      " |              the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to `True`\n",
      " |              when there is some disk offload.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      " |              weights. If set to `False`, safetensors weights are not loaded.\n",
      " |          use_onnx (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
      " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
      " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
      " |              with `.onnx` and `.pb`.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
      " |              class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
      " |              below for more information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models) models, log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      \n",
      " |      >>> # Download pipeline from huggingface.co and cache.\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
      " |      \n",
      " |      >>> # Download pipeline that requires an authorization token\n",
      " |      >>> # For more information on access tokens, please refer to this section\n",
      " |      >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      \n",
      " |      >>> # Use a different scheduler\n",
      " |      >>> from diffusers import LMSDiscreteScheduler\n",
      " |      \n",
      " |      >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
      " |      >>> pipeline.scheduler = scheduler\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  numpy_to_pil(images)\n",
      " |      Convert a NumPy image or a batch of images to a PIL image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  components\n",
      " |      The `self.components` property can be useful to run different pipelines with the same weights and\n",
      " |      configurations without reallocating additional memory.\n",
      " |      \n",
      " |      Returns (`dict`):\n",
      " |          A dictionary containing all the modules needed to initialize the pipeline.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import (\n",
      " |      ...     StableDiffusionPipeline,\n",
      " |      ...     StableDiffusionImg2ImgPipeline,\n",
      " |      ...     StableDiffusionInpaintPipeline,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> text2img = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
      " |      >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n",
      " |      ```\n",
      " |  \n",
      " |  device\n",
      " |      Returns:\n",
      " |          `torch.device`: The torch device on which the pipeline is located.\n",
      " |  \n",
      " |  name_or_path\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  config_name = 'model_index.json'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      The only reason we overwrite `getattr` here is to gracefully deprecate accessing\n",
      " |      config attributes directly. See https://github.com/huggingface/diffusers/pull/3129\n",
      " |      \n",
      " |      Tihs funtion is mostly copied from PyTorch's __getattr__ overwrite:\n",
      " |      https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  register_to_config(self, **kwargs)\n",
      " |  \n",
      " |  save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      " |      Save a configuration object to the directory specified in `save_directory` so that it can be reloaded using the\n",
      " |      [`~ConfigMixin.from_config`] class method.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the configuration JSON file is saved (will be created if it does not exist).\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face Hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save the configuration instance's parameters to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file to save a configuration instance's parameters.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes the configuration instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`:\n",
      " |              String containing all the attributes that make up the configuration instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  extract_init_dict(config_dict, **kwargs) from builtins.type\n",
      " |  \n",
      " |  from_config(config: Union[diffusers.configuration_utils.FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs) from builtins.type\n",
      " |      Instantiate a Python class from a config dictionary.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          config (`Dict[str, Any]`):\n",
      " |              A config dictionary from which the Python class is instantiated. Make sure to only load configuration\n",
      " |              files of compatible classes.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether kwargs that are not consumed by the Python class should be returned or not.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it is loaded) and initiate the Python class.\n",
      " |              `**kwargs` are passed directly to the underlying scheduler/model's `__init__` method and eventually\n",
      " |              overwrite the same named arguments in `config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`ModelMixin`] or [`SchedulerMixin`]:\n",
      " |              A model or scheduler object instantiated from a config dictionary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n",
      " |      \n",
      " |      >>> # Download scheduler from huggingface.co and cache.\n",
      " |      >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n",
      " |      \n",
      " |      >>> # Instantiate DDIM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n",
      " |      \n",
      " |      >>> # Instantiate PNDM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n",
      " |      ```\n",
      " |  \n",
      " |  get_config_dict(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  load_config(pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, return_commit_hash=False, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      Load a model or scheduler configuration.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with\n",
      " |                    [`~ConfigMixin.save_config`].\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False):\n",
      " |              Whether unused keyword arguments of the config are returned.\n",
      " |          return_commit_hash (`bool`, *optional*, defaults to `False):\n",
      " |              Whether the `commit_hash` of the loaded configuration are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict`:\n",
      " |              A dictionary of all the parameters stored in a JSON configuration file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  config\n",
      " |      Returns the config of the class as a frozen dictionary\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Config of the class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  has_compatibles = False\n",
      " |  \n",
      " |  ignore_for_config = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.utils.hub_utils.PushToHubMixin:\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Optional[str] = None, create_pr: bool = False, safe_serialization: bool = True, variant: Optional[str] = None) -> str\n",
      " |      Upload model, scheduler, or pipeline files to the ð¤ Hugging Face Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your model, scheduler, or pipeline files to. It should\n",
      " |              contain your organization name when pushing to an organization. `repo_id` can also be a path to a local\n",
      " |              directory.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Default to `\"Upload {object}\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          token (`str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. The token generated when running\n",
      " |              `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights to the `safetensors` format.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from diffusers import UNet2DConditionModel\n",
      " |      \n",
      " |      unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\")\n",
      " |      \n",
      " |      # Push the `unet` to your namespace with the name \"my-finetuned-unet\".\n",
      " |      unet.push_to_hub(\"my-finetuned-unet\")\n",
      " |      \n",
      " |      # Push the `unet` to an organization with the name \"my-finetuned-unet\".\n",
      " |      unet.push_to_hub(\"your-org/my-finetuned-unet\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.TextualInversionLoaderMixin:\n",
      " |  \n",
      " |  load_textual_inversion(self, pretrained_model_name_or_path: Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]], token: Union[str, List[str], NoneType] = None, **kwargs)\n",
      " |      Load textual inversion embeddings into the text encoder of [`StableDiffusionPipeline`] (both ð¤ Diffusers and\n",
      " |      Automatic1111 formats are supported).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike` or `List[str or os.PathLike]` or `Dict` or `List[Dict]`):\n",
      " |              Can be either one of the following or a list of them:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`) of a\n",
      " |                    pretrained model hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_text_inversion_directory/`) containing the textual\n",
      " |                    inversion weights.\n",
      " |                  - A path to a *file* (for example `./my_text_inversions.pt`) containing textual inversion weights.\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          token (`str` or `List[str]`, *optional*):\n",
      " |              Override the token to use for the textual inversion weights. If `pretrained_model_name_or_path` is a\n",
      " |              list, then `token` must also be a list of equal length.\n",
      " |          weight_name (`str`, *optional*):\n",
      " |              Name of a custom weight file. This should be used when:\n",
      " |      \n",
      " |                  - The saved textual inversion file is in ð¤ Diffusers format, but was saved under a specific weight\n",
      " |                    name such as `text_inv.bin`.\n",
      " |                  - The saved textual inversion file is in the Automatic1111 format.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in ð¤ Diffusers format:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
      " |      \n",
      " |      prompt = \"A <cat-toy> backpack\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"cat-backpack.png\")\n",
      " |      ```\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in Automatic1111 format, make sure to download the vector first\n",
      " |      (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857)) and then load the vector\n",
      " |      locally:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n",
      " |      \n",
      " |      prompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"character.png\")\n",
      " |      ```\n",
      " |  \n",
      " |  maybe_convert_prompt(self, prompt: Union[str, List[str]], tokenizer: 'PreTrainedTokenizer')\n",
      " |      Processes prompts that include a special token corresponding to a multi-vector textual inversion embedding to\n",
      " |      be replaced with multiple special tokens each corresponding to one of the vectors. If the prompt has no textual\n",
      " |      inversion token or if the textual inversion token is a single vector, the input prompt is returned.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prompt (`str` or list of `str`):\n",
      " |              The prompt or prompts to guide the image generation.\n",
      " |          tokenizer (`PreTrainedTokenizer`):\n",
      " |              The tokenizer responsible for encoding the prompt into input tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or list of `str`: The converted prompt\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  load_lora_weights(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs)\n",
      " |      Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and\n",
      " |      `self.text_encoder`.\n",
      " |      \n",
      " |      All kwargs are forwarded to `self.lora_state_dict`.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into\n",
      " |      `self.unet`.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded\n",
      " |      into `self.text_encoder`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              See [`~loaders.LoraLoaderMixin.lora_state_dict`].\n",
      " |          kwargs (`dict`, *optional*):\n",
      " |              See [`~loaders.LoraLoaderMixin.lora_state_dict`].\n",
      " |  \n",
      " |  unload_lora_weights(self)\n",
      " |      Unloads the LoRA parameters.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> # Assuming `pipeline` is already loaded with the LoRA parameters.\n",
      " |      >>> pipeline.unload_lora_weights()\n",
      " |      >>> ...\n",
      " |      ```\n",
      " |  \n",
      " |  write_lora_layers(state_dict: Dict[str, torch.Tensor], save_directory: str, is_main_process: bool, weight_name: str, save_function: Callable, safe_serialization: bool)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  load_lora_into_text_encoder(state_dict, network_alphas, text_encoder, prefix=None, lora_scale=1.0) from builtins.type\n",
      " |      This will load the LoRA layers specified in `state_dict` into `text_encoder`\n",
      " |      \n",
      " |      Parameters:\n",
      " |          state_dict (`dict`):\n",
      " |              A standard state dict containing the lora layer parameters. The key should be prefixed with an\n",
      " |              additional `text_encoder` to distinguish between unet lora layers.\n",
      " |          network_alphas (`Dict[str, float]`):\n",
      " |              See `LoRALinearLayer` for more details.\n",
      " |          text_encoder (`CLIPTextModel`):\n",
      " |              The text encoder model to load the LoRA layers into.\n",
      " |          prefix (`str`):\n",
      " |              Expected prefix of the `text_encoder` in the `state_dict`.\n",
      " |          lora_scale (`float`):\n",
      " |              How much to scale the output of the lora linear layer before it is added with the output of the regular\n",
      " |              lora layer.\n",
      " |  \n",
      " |  load_lora_into_unet(state_dict, network_alphas, unet) from builtins.type\n",
      " |      This will load the LoRA layers specified in `state_dict` into `unet`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          state_dict (`dict`):\n",
      " |              A standard state dict containing the lora layer parameters. The keys can either be indexed directly\n",
      " |              into the unet or prefixed with an additional `unet` which can be used to distinguish between text\n",
      " |              encoder lora layers.\n",
      " |          network_alphas (`Dict[str, float]`):\n",
      " |              See `LoRALinearLayer` for more details.\n",
      " |          unet (`UNet2DConditionModel`):\n",
      " |              The UNet model to load the LoRA layers into.\n",
      " |  \n",
      " |  lora_state_dict(pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs) from builtins.type\n",
      " |      Return state dict for lora weights and the network alphas.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      We support loading A1111 formatted LoRA checkpoints in a limited capacity.\n",
      " |      \n",
      " |      This function is experimental and might change in the future.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved\n",
      " |                    with [`ModelMixin.save_pretrained`].\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |  \n",
      " |  save_lora_weights(save_directory: Union[str, os.PathLike], unet_lora_layers: Dict[str, Union[torch.nn.modules.module.Module, torch.Tensor]] = None, text_encoder_lora_layers: Dict[str, torch.nn.modules.module.Module] = None, is_main_process: bool = True, weight_name: str = None, save_function: Callable = None, safe_serialization: bool = True) from builtins.type\n",
      " |      Save the LoRA parameters corresponding to the UNet and text encoder.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save LoRA parameters to. Will be created if it doesn't exist.\n",
      " |          unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `unet`.\n",
      " |          text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text\n",
      " |              encoder LoRA state dict because it comes from ð¤ Transformers.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful during distributed training and you\n",
      " |              need to call this function on all processes. In this case, set `is_main_process=True` only on the main\n",
      " |              process to avoid race conditions.\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful during distributed training when you need to\n",
      " |              replace `torch.save` with another method. Can be configured with the environment variable\n",
      " |              `DIFFUSERS_SAVE_MODE`.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  lora_scale\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  text_encoder_name = 'text_encoder'\n",
      " |  \n",
      " |  unet_name = 'unet'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TextToVideoSDPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71bea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
